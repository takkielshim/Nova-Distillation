#!/usr/bin/env python3
"""
LLM-as-a-Judge Evaluation Script
Configurable generator and evaluator models for performance assessment
"""

import json
import boto3
import time
import os
from typing import List, Dict, Any, Optional
from datetime import datetime
import csv


class LLMJudgeEvaluator:
    """Evaluates model responses using LLM-as-a-Judge methodology"""
    
    def __init__(self, region: str = 'us-east-1', generator_model: str = 'nova-pro', 
                 evaluator_model: str = 'claude-sonnet-3.7', custom_model_id: Optional[str] = None,
                 max_tokens: int = 400, temperature: float = 0.3, top_p: float = 0.8):
        """
        Initialize the evaluator
        
        Args:
            region: AWS region for Bedrock service
            generator_model: Model to generate responses ('nova-pro', 'nova-lite', 'nova-custom')
            evaluator_model: Model to evaluate responses ('claude-sonnet-3.7')
            custom_model_id: ARN/ID for custom model (required if generator_model is 'nova-custom')
            max_tokens: Maximum tokens for response generation
            temperature: Temperature for response generation
            top_p: Top-p for response generation
        """
        self.bedrock_client = boto3.client('bedrock-runtime', region_name=region)
        self.region = region
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.top_p = top_p
        
        # Configure generator model
        self._setup_generator_model(generator_model, custom_model_id)
        
        # Configure evaluator model
        self._setup_evaluator_model(evaluator_model)
        
    def _setup_generator_model(self, generator_model: str, custom_model_id: Optional[str]):
        """Setup generator model configuration"""
        generator_model = generator_model.lower()
        
        if generator_model == 'nova-pro':
            self.nova_model_id = 'us.amazon.nova-pro-v1:0'
            self.generator_name = 'Amazon Nova Pro'
        elif generator_model == 'nova-lite':
            self.nova_model_id = 'us.amazon.nova-lite-v1:0'
            self.generator_name = 'Amazon Nova Lite'
        elif generator_model in ['nova-custom', 'custom']:
            if not custom_model_id:
                raise ValueError("custom_model_id is required when using custom generator model")
            self.nova_model_id = custom_model_id
            self.generator_name = f'Custom Nova Model ({custom_model_id.split("/")[-1] if "/" in custom_model_id else custom_model_id})'
        else:
            raise ValueError(f"Unsupported generator model: {generator_model}. Use 'nova-pro', 'nova-lite', or 'nova-custom'")
        
        self.generator_model = generator_model
    
    def _setup_evaluator_model(self, evaluator_model: str):
        """Setup evaluator model configuration"""
        evaluator_model = evaluator_model.lower()
        
        if evaluator_model in ['claude-sonnet-3.7', 'sonnet-3.7', 'claude-sonnet']:
            self.evaluator_model_id = 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'
            self.evaluator_name = 'Claude Sonnet 3.7'
        else:
            raise ValueError(f"Unsupported evaluator model: {evaluator_model}. Currently only 'claude-sonnet-3.7' is supported")
    
    def load_prompts(self, file_path: str) -> List[str]:
        """
        Load prompts from JSONL file
        
        Args:
            file_path: Path to the JSONL file containing prompts
            
        Returns:
            List of prompt strings
        """
        prompts = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line:
                        try:
                            data = json.loads(line)
                            if 'prompt' in data:
                                prompts.append(data['prompt'])
                            else:
                                print(f"Warning: Line {line_num} missing 'prompt' field")
                        except json.JSONDecodeError as e:
                            print(f"Warning: Invalid JSON on line {line_num}: {e}")
        except FileNotFoundError:
            print(f"Error: File not found: {file_path}")
            return []
        except Exception as e:
            print(f"Error loading prompts: {e}")
            return []
        
        return prompts
    
    def generate_response_nova(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """
        Generate response using Amazon Nova models
        
        Args:
            prompt: User prompt
            system_prompt: Optional system prompt for response formatting
            
        Returns:
            Generated response text
        """
        try:
            # Default system prompt for Korean e-commerce responses
            if system_prompt is None:
                system_prompt = """ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ë©´ì„œë„ ì™„ì „í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì¤‘ìš”í•œ ì§€ì¹¨:
- ë‹µë³€ì€ 150-200ê¸€ì ì •ë„ë¡œ ì‘ì„±
- ë°˜ë“œì‹œ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ë§ˆë¬´ë¦¬
- í•µì‹¬ ì •ë³´ë§Œ í¬í•¨í•˜ì—¬ ê°„ê²°í•˜ê²Œ ì‘ì„±
- ë¬¸ì¥ì´ ì¤‘ê°„ì— ëŠê¸°ì§€ ì•Šë„ë¡ ì£¼ì˜

ì§ˆë¬¸: {prompt}

ì™„ì„±ëœ ë‹µë³€:"""
            
            # Format the prompt
            formatted_prompt = system_prompt.format(prompt=prompt)
            
            # Nova request payload
            payload = {
                "messages": [
                    {
                        "role": "user",
                        "content": [{"text": formatted_prompt}]
                    }
                ],
                "inferenceConfig": {
                    "maxTokens": self.max_tokens,
                    "temperature": self.temperature,
                    "topP": self.top_p
                }
            }
            
            response = self.bedrock_client.invoke_model(
                modelId=self.nova_model_id,
                body=json.dumps(payload),
                contentType='application/json'
            )
            
            result = json.loads(response['body'].read())
            generated_response = result['output']['message']['content'][0]['text'].strip()
            
            # Post-process response to ensure completeness
            generated_response = self._post_process_response(generated_response)
            
            return generated_response
            
        except Exception as e:
            print(f"Error generating with {self.generator_name}: {e}")
            return f"Error: {str(e)}"
    
    def _post_process_response(self, response: str, max_length: int = 250) -> str:
        """
        Post-process generated response to ensure completeness and appropriate length
        
        Args:
            response: Raw generated response
            max_length: Maximum allowed response length
            
        Returns:
            Post-processed response
        """
        # Check if response ends properly
        proper_endings = ['.', '!', '?', 'ë‹¤', 'ìŠµë‹ˆë‹¤', 'ë©ë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤']
        
        if not any(response.endswith(ending) for ending in proper_endings):
            # Try to find last complete sentence
            for delimiter in ['.', '!', '?']:
                if delimiter in response:
                    parts = response.split(delimiter)
                    if len(parts) > 1:
                        # Keep complete sentences only
                        complete_parts = parts[:-1] if len(parts[-1].strip()) < 20 else parts
                        response = delimiter.join(complete_parts)
                        if not response.endswith(delimiter):
                            response += delimiter
                        break
            
            # Check for Korean sentence endings
            korean_endings = ['ë‹¤.', 'ìŠµë‹ˆë‹¤.', 'ë©ë‹ˆë‹¤.', 'ìˆìŠµë‹ˆë‹¤.', 'ì—†ìŠµë‹ˆë‹¤.', 'í•©ë‹ˆë‹¤.']
            for ending in korean_endings:
                if ending in response:
                    idx = response.rfind(ending)
                    if idx != -1:
                        response = response[:idx + len(ending)]
                        break
            else:
                # If no proper ending found, add period
                if len(response) > 50:
                    words = response.split()
                    if len(words) > 10:
                        target_length = min(200, len(response))
                        truncated = ""
                        for word in words:
                            if len(truncated + word + ' ') <= target_length - 10:
                                truncated += word + ' '
                            else:
                                break
                        response = truncated.strip() + '.'
        
        # Truncate if too long
        if len(response) > max_length:
            sentences = []
            current_length = 0
            
            for delimiter in ['.', '!', '?']:
                if delimiter in response:
                    parts = response.split(delimiter)
                    for part in parts[:-1]:  # Exclude last incomplete part
                        sentence = part + delimiter
                        if current_length + len(sentence) <= max_length - 20:
                            sentences.append(sentence)
                            current_length += len(sentence)
                        else:
                            break
                    break
            
            if sentences:
                response = ''.join(sentences)
        
        return response
    
    def evaluate_with_claude(self, prompt: str, response: str, 
                           evaluation_criteria: Optional[Dict[str, str]] = None) -> Dict[str, Any]:
        """
        Evaluate response using Claude Sonnet
        
        Args:
            prompt: Original prompt
            response: Generated response to evaluate
            evaluation_criteria: Custom evaluation criteria
            
        Returns:
            Evaluation results dictionary
        """
        # Default evaluation criteria
        if evaluation_criteria is None:
            evaluation_criteria = {
                "accuracy": "ë‹µë³€ì˜ ì‚¬ì‹¤ì  ì •í™•ì„±ê³¼ ì§ˆë¬¸ ì´í•´ë„",
                "completeness": "ë‹µë³€ì´ ì¶©ë¶„íˆ ìƒì„¸í•˜ê³  ì™„ì „í•œê°€",
                "relevance": "ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ê°€",
                "helpfulness": "ë‹µë³€ì´ ì‚¬ìš©ìì—ê²Œ ì‹¤ì§ˆì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ”ê°€",
                "language_quality": "í•œêµ­ì–´ í‘œí˜„ì´ ìì—°ìŠ¤ëŸ½ê³  ì ì ˆí•œê°€"
            }
        
        # Build evaluation prompt
        criteria_text = ""
        for i, (criterion, description) in enumerate(evaluation_criteria.items(), 1):
            criteria_text += f"{i}. **{criterion.replace('_', ' ').title()} ({criterion.title()})**: {description} (1-5ì )\n"
            criteria_text += self._get_scoring_guide(criterion) + "\n\n"
        
        evaluation_prompt = f"""
ë‹¤ìŒì€ í•œêµ­ì–´ ì‡¼í•‘/ì „ììƒê±°ë˜ ê´€ë ¨ ì§ˆë¬¸ê³¼ AIì˜ ë‹µë³€ì…ë‹ˆë‹¤. ì´ ë‹µë³€ì„ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:

**ì›ë³¸ ì§ˆë¬¸:** {prompt}

**AI ë‹µë³€:** {response}

**í‰ê°€ ê¸°ì¤€:**
{criteria_text}

**ì‘ë‹µ í˜•ì‹:**
```json
{{
    {', '.join([f'"{k}": <ì ìˆ˜>' for k in evaluation_criteria.keys()])},
    "overall_score": <ì „ì²´ í‰ê·  ì ìˆ˜>,
    "feedback": "<êµ¬ì²´ì ì¸ í”¼ë“œë°± ë° ê°œì„ ì >"
}}
```

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""
        
        try:
            payload = {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 1000,
                "temperature": 0.3,
                "top_p": 0.9,
                "messages": [
                    {
                        "role": "user",
                        "content": evaluation_prompt
                    }
                ]
            }
            
            response_obj = self.bedrock_client.invoke_model(
                modelId=self.evaluator_model_id,
                body=json.dumps(payload),
                contentType='application/json'
            )
            
            result = json.loads(response_obj['body'].read())
            evaluation_text = result['content'][0]['text']
            
            # Extract JSON part
            start_idx = evaluation_text.find('{')
            end_idx = evaluation_text.rfind('}') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = evaluation_text[start_idx:end_idx]
                return json.loads(json_str)
            else:
                return {"error": "Could not parse evaluation JSON"}
                
        except Exception as e:
            print(f"Error evaluating with {self.evaluator_name}: {e}")
            return {"error": str(e)}
    
    def _get_scoring_guide(self, criterion: str) -> str:
        """Get scoring guide for specific criterion"""
        guides = {
            "accuracy": """   - 5ì : ì§ˆë¬¸ì„ ì™„ë²½íˆ ì´í•´í•˜ê³  ì‚¬ì‹¤ì ìœ¼ë¡œ ì •í™•í•œ ì •ë³´ ì œê³µ
   - 4ì : ì§ˆë¬¸ì„ ì˜ ì´í•´í•˜ê³  ëŒ€ë¶€ë¶„ ì •í™•í•œ ì •ë³´ ì œê³µ, ì‚¬ì†Œí•œ ì˜¤ë¥˜ ìˆì„ ìˆ˜ ìˆìŒ
   - 3ì : ì§ˆë¬¸ì„ ì–´ëŠ ì •ë„ ì´í•´í•˜ê³  ë¶€ë¶„ì ìœ¼ë¡œ ì •í™•í•œ ì •ë³´ ì œê³µ
   - 2ì : ì§ˆë¬¸ ì´í•´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ë¶€ì •í™•í•œ ì •ë³´ê°€ ë§ìŒ
   - 1ì : ì§ˆë¬¸ì„ ì˜ëª» ì´í•´í–ˆê±°ë‚˜ ëª…ë°±íˆ ì˜ëª»ëœ ì •ë³´ ì œê³µ""",
            
            "completeness": """   - 5ì : ì§ˆë¬¸ì— ëŒ€í•œ ëª¨ë“  ì¸¡ë©´ì„ í¬ê´„ì ìœ¼ë¡œ ë‹¤ë£¸
   - 4ì : ì£¼ìš” ë‚´ìš©ì„ ì˜ ë‹¤ë£¨ì§€ë§Œ ì¼ë¶€ ì„¸ë¶€ì‚¬í•­ ëˆ„ë½
   - 3ì : ê¸°ë³¸ì ì¸ ë‚´ìš©ì€ í¬í•¨í•˜ì§€ë§Œ ë¶ˆì™„ì „í•¨
   - 2ì : ì¤‘ìš”í•œ ì •ë³´ê°€ ë§ì´ ëˆ„ë½ë¨
   - 1ì : ë§¤ìš° ë¶ˆì™„ì „í•˜ê±°ë‚˜ í•µì‹¬ ë‚´ìš© ëˆ„ë½""",
            
            "relevance": """   - 5ì : ì§ˆë¬¸ê³¼ ì™„ì „íˆ ê´€ë ¨ëœ ë‚´ìš©ë§Œ í¬í•¨
   - 4ì : ëŒ€ë¶€ë¶„ ê´€ë ¨ì„± ìˆì§€ë§Œ ì¼ë¶€ ë¶ˆí•„ìš”í•œ ë‚´ìš© í¬í•¨
   - 3ì : ì–´ëŠ ì •ë„ ê´€ë ¨ì„± ìˆì§€ë§Œ ì£¼ì œì—ì„œ ë²—ì–´ë‚œ ë¶€ë¶„ ìˆìŒ
   - 2ì : ê´€ë ¨ì„±ì´ ë‚®ê±°ë‚˜ ì£¼ì œì™€ ë§ì§€ ì•ŠëŠ” ë‚´ìš©ì´ ë§ìŒ
   - 1ì : ì§ˆë¬¸ê³¼ ê±°ì˜ ê´€ë ¨ ì—†ëŠ” ë‹µë³€""",
            
            "helpfulness": """   - 5ì : ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ì‹¤ìš©ì ìœ¼ë¡œ ì œê³µ
   - 4ì : ëŒ€ì²´ë¡œ ìœ ìš©í•˜ì§€ë§Œ ì¼ë¶€ ê°œì„  ì—¬ì§€ ìˆìŒ
   - 3ì : ì–´ëŠ ì •ë„ ë„ì›€ì´ ë˜ì§€ë§Œ ì œí•œì ì„
   - 2ì : ë„ì›€ì´ ë˜ëŠ” ì •ë„ê°€ ë‚®ìŒ
   - 1ì : ê±°ì˜ ë„ì›€ì´ ë˜ì§€ ì•ŠìŒ""",
            
            "language_quality": """   - 5ì : ì™„ë²½í•œ í•œêµ­ì–´ í‘œí˜„, ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰¬ì›€
   - 4ì : ëŒ€ì²´ë¡œ ìì—°ìŠ¤ëŸ½ì§€ë§Œ ì‚¬ì†Œí•œ ì–´ìƒ‰í•¨ ìˆì„ ìˆ˜ ìˆìŒ
   - 3ì : ì´í•´ ê°€ëŠ¥í•˜ì§€ë§Œ ì–´ìƒ‰í•œ í‘œí˜„ì´ë‚˜ ë¬¸ë²• ì˜¤ë¥˜ ìˆìŒ
   - 2ì : ë¶€ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì´ ë§ê³  ì´í•´í•˜ê¸° ì–´ë ¤ìš´ ë¶€ë¶„ ìˆìŒ
   - 1ì : ë§¤ìš° ì–´ìƒ‰í•˜ê±°ë‚˜ ì´í•´í•˜ê¸° ì–´ë ¤ìš´ í•œêµ­ì–´"""
        }
        
        return guides.get(criterion, "   - 1-5ì  ì²™ë„ë¡œ í‰ê°€")
    
    def run_evaluation(self, eval_file_path: str, output_file: Optional[str] = None, 
                      rate_limit_delay: float = 1.0) -> Optional[Dict[str, Any]]:
        """
        Run complete evaluation pipeline
        
        Args:
            eval_file_path: Path to evaluation file
            output_file: Output file path (auto-generated if None)
            rate_limit_delay: Delay between API calls to avoid rate limits
            
        Returns:
            Evaluation results dictionary
        """
        print("ğŸš€ Starting LLM-as-a-Judge Evaluation")
        print(f"Generator: {self.generator_name}")
        print(f"Evaluator: {self.evaluator_name}")
        print(f"Region: {self.region}")
        print("-" * 60)
        
        # Load prompts
        prompts = self.load_prompts(eval_file_path)
        if not prompts:
            print("âŒ No prompts loaded. Exiting.")
            return None
        
        print(f"ğŸ“ Loaded {len(prompts)} prompts")
        
        results = []
        total_scores = {
            'accuracy': 0,
            'completeness': 0,
            'relevance': 0,
            'helpfulness': 0,
            'language_quality': 0,
            'overall_score': 0
        }
        
        for i, prompt in enumerate(prompts, 1):
            print(f"\nğŸ”„ Processing {i}/{len(prompts)}: {prompt[:50]}...")
            
            # Generate response
            print(f"  ğŸ“¤ Generating response with {self.generator_name}...")
            response = self.generate_response_nova(prompt)
            
            # Rate limiting
            time.sleep(rate_limit_delay)
            
            # Evaluate response
            print(f"  ğŸ“Š Evaluating with {self.evaluator_name}...")
            evaluation = self.evaluate_with_claude(prompt, response)
            
            # Rate limiting
            time.sleep(rate_limit_delay)
            
            result = {
                'prompt_id': i,
                'prompt': prompt,
                'response': response,
                'evaluation': evaluation,
                'timestamp': datetime.utcnow().isoformat()
            }
            
            results.append(result)
            
            # Accumulate scores if evaluation was successful
            if 'error' not in evaluation:
                for key in total_scores:
                    if key in evaluation:
                        total_scores[key] += evaluation[key]
                
                print(f"  âœ… Overall Score: {evaluation.get('overall_score', 'N/A')}/5")
            else:
                print(f"  âŒ Evaluation Error: {evaluation['error']}")
        
        # Calculate averages
        valid_evaluations = len([r for r in results if 'error' not in r['evaluation']])
        if valid_evaluations > 0:
            avg_scores = {k: v / valid_evaluations for k, v in total_scores.items()}
        else:
            avg_scores = {k: 0 for k in total_scores}
        
        # Generate output filename if not provided
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = self.generator_model.replace('-', '_')
            output_file = f"evaluation_results_{model_name}_{timestamp}.json"
        
        # Prepare final results
        final_results = {
            'metadata': {
                'generator_model': self.nova_model_id,
                'generator_name': self.generator_name,
                'evaluator_model': self.evaluator_model_id,
                'evaluator_name': self.evaluator_name,
                'region': self.region,
                'total_prompts': len(prompts),
                'valid_evaluations': valid_evaluations,
                'evaluation_date': datetime.utcnow().isoformat(),
                'parameters': {
                    'max_tokens': self.max_tokens,
                    'temperature': self.temperature,
                    'top_p': self.top_p,
                    'rate_limit_delay': rate_limit_delay
                }
            },
            'average_scores': avg_scores,
            'detailed_results': results
        }
        
        # Save results
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(final_results, f, ensure_ascii=False, indent=2)
            
            # Save CSV summary
            csv_file = output_file.replace('.json', '_summary.csv')
            self.save_csv_summary(results, csv_file)
            
            # Print summary
            self._print_summary(len(prompts), valid_evaluations, avg_scores, output_file, csv_file)
            
            return final_results
            
        except Exception as e:
            print(f"âŒ Error saving results: {e}")
            return final_results
    
    def _print_summary(self, total_prompts: int, valid_evaluations: int, 
                      avg_scores: Dict[str, float], output_file: str, csv_file: str):
        """Print evaluation summary"""
        print("\n" + "="*70)
        print("ğŸ“Š EVALUATION SUMMARY")
        print("="*70)
        print(f"Generator: {self.generator_name}")
        print(f"Evaluator: {self.evaluator_name}")
        print(f"Total Prompts: {total_prompts}")
        print(f"Valid Evaluations: {valid_evaluations}")
        print(f"Success Rate: {valid_evaluations/total_prompts*100:.1f}%")
        print("\nğŸ“ˆ Average Scores:")
        for metric, score in avg_scores.items():
            print(f"  {metric.replace('_', ' ').title()}: {score:.2f}/5.0")
        
        print(f"\nğŸ’¾ Results saved to:")
        print(f"  - Detailed: {output_file}")
        print(f"  - Summary: {csv_file}")
    
    def save_csv_summary(self, results: List[Dict], csv_file: str):
        """Save evaluation summary to CSV"""
        try:
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                
                # Header
                writer.writerow([
                    'Prompt ID', 'Prompt', 'Response Length', 
                    'Accuracy', 'Completeness', 'Relevance', 
                    'Helpfulness', 'Language Quality', 'Overall Score',
                    'Has Error', 'Feedback'
                ])
                
                # Data rows
                for result in results:
                    evaluation = result['evaluation']
                    has_error = 'error' in evaluation
                    
                    writer.writerow([
                        result['prompt_id'],
                        result['prompt'][:100] + '...' if len(result['prompt']) > 100 else result['prompt'],
                        len(result['response']),
                        evaluation.get('accuracy', 'N/A'),
                        evaluation.get('completeness', 'N/A'),
                        evaluation.get('relevance', 'N/A'),
                        evaluation.get('helpfulness', 'N/A'),
                        evaluation.get('language_quality', 'N/A'),
                        evaluation.get('overall_score', 'N/A'),
                        has_error,
                        evaluation.get('feedback', '')[:200] + '...' if len(evaluation.get('feedback', '')) > 200 else evaluation.get('feedback', '')
                    ])
        except Exception as e:
            print(f"Warning: Could not save CSV summary: {e}")


def main():
    """Main execution function for standalone usage"""
    import argparse
    
    parser = argparse.ArgumentParser(description='LLM-as-a-Judge Evaluation')
    parser.add_argument('--file', default='test.jsonl', help='Evaluation file path')
    parser.add_argument('--generator', default='nova-pro', choices=['nova-pro', 'nova-lite', 'nova-custom'],
                       help='Generator model')
    parser.add_argument('--custom-model-id', help='Custom model ID for nova-custom')
    parser.add_argument('--region', default='us-east-1', help='AWS region')
    parser.add_argument('--output', help='Output file path')
    
    args = parser.parse_args()
    
    # Validate custom model requirements
    if args.generator == 'nova-custom' and not args.custom_model_id:
        print("âŒ --custom-model-id is required when using nova-custom generator")
        return
    
    # Initialize evaluator
    evaluator = LLMJudgeEvaluator(
        region=args.region,
        generator_model=args.generator,
        custom_model_id=args.custom_model_id
    )
    
    # Run evaluation
    results = evaluator.run_evaluation(args.file, args.output)
    
    if results:
        print("\nğŸ‰ Evaluation completed successfully!")
    else:
        print("\nâŒ Evaluation failed!")


if __name__ == "__main__":
    main()
